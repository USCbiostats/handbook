[
["index.html", "USC biostats R Handbook 1 Welcome 1.1 add chapter 1.2 editing a chapter 1.3 compile handbook", " USC biostats R Handbook Emil Hvitfeldt Spring 2019 1 Welcome If you find any mistake or have other contributions you would like added please follow steps outlined below. For bigger additions add an issue beforehand to get approved. 1.1 add chapter To add a chapter create a file called chapternumber_chaptername.Rmd (for example 01_Introduction.Rmd). 1.2 editing a chapter Before you begin, make sure you: fork the repo, click “Clone or Download” button and copy the link Open R Studio and Click File&lt; New Project&lt;Version Control&lt;Git Copy the link into the top box, name the copied repository whatever you would like, and select a location for the repository to be stored Edit the chapter you want, then save it In the R Console run the following: bookdown::render_book(&quot;index.Rmd&quot;,&quot;bookdown::gitbook&quot;) Click on the Git tab in the top right window of your R Studio Click Commit Click on the files you would like to commit Add a commit message then click Commit Click on the green arrow to push to github Submit a pull request to the main repo so your changes can be incorporated 1.3 compile handbook After updating, be sure to compile the handbook by setting your working directory and running bookdown::render_book(&quot;index.Rmd&quot;, &quot;bookdown::gitbook&quot;) "],
["coding-standards.html", "2 Coding Standards 2.1 General overview 2.2 Helpful suggestions", " 2 Coding Standards This chapter will outline standards we aim to adhere to. In addition to these standards will be some suggestions on best practices we hopes you will find useful. 2.1 General overview Try to keep your code to less then 80 characters per line. This will improve visibility as the code willm fit on your screen when you are working. If you are using Rstudio, you can add a visual marker on the 80 character line. Click Rstudio -&gt; Preferences -&gt; Code -&gt; display -&gt; Show margin. document your code. Explain yourself. Each function should have acommpanying documentation explaning what it is doing. This will help other people understand what your code is doing without having to look at the code indside the function. Notice that “other people” also describes “future you”. Use white space for indenting, 2 characters. When possible, structure your code as sections/files, with files holding similar functions and sections to give internal structure to your file. Include a informative README in each project. 2.2 Helpful suggestions Use a integrated development environment (IDE). We recommend https://www.rstudio.com/. TODO explain what the benefits of using a IDE. If you are looking for a coding style, then the The tidyverse style guide is a good place to start. "],
["building-a-r-package.html", "3 Building a R package 3.1 Motivation 3.2 Preparation 3.3 Minimal R Package 3.4 Additional Components 3.5 Resources", " 3 Building a R package 3.1 Motivation This chapter will show you how to create a R package with varying number of additions. From a simple minimal package for personal use to a fully-fleshed package. Creating a R package is not only done for publication. It also helps you stay organizes, save yourself time while still letting you share your code with other people. Learning what goes into creating a package can be a mouthful at first but the conventions and rules revolving around packages makes the creation easy. 3.2 Preparation To get started it would be preferable for you to have git installed on your machine and have a Github account. https://happygitwithr.com/ provides a brilliant and thorough walk-through of using git/Github with R. In addition to git, you will also need the following packages install.packages(c(&quot;devtools&quot;, &quot;roxygen2&quot;, &quot;testthat&quot;, &quot;knitr&quot;)) If you are planning on using compiled code you will need to install the following depending on your operating system: On Windows, download and install Rtools. On Mac, download and install XCode (available for free in the App Store) or the Command Line Tools for Xcode. On Linux, download and install the R development tools. 3.2.1 Naming your R package When creating a package you need to give it at name. The name can only consist of letters, numbers and periods must start with a letter, and cannot end with a period Furthermore we recommend that you don’t use periods and stick with letters as much as possible. You need a unique name, mainly because of the fact that if you plan on getting your package on CRAN or bioconductor you can’t overlap with any of their names. A easy check for can be done using the available package. available also checks across acronyms, slang and other meanings that you might have missed. Secondly try to pick a name that is googleable. 3.2.2 Checking How that you are a R developer you will benefit from using the essential tool R CMD check. R CMD check check your package for common problems and reports them back to you. Using this early and often stops you from having to deal with ingrown problems. You can run R CMD check by typing the code devtools::check() or with the shortcut Ctrl/Cmd + Shift + E in RStudio. A common workflow is Run R CMD check If there is a problem fix one of them Repeat until you don’t have any errors More information about individual checks can be found here. Further reading: https://r-pkgs.org/check 3.3 Minimal R Package Creating a package can easily be done using the usethis package, all you need is to provide a path to the directory you want the package to be created in. create_package(&quot;~/path/to/packagename&quot;) If you are using Rstudio you should already be transported to a project at the specified location with the following content For now we will focus on the /R folder and the DESCRIPTION files as the remaining files/folders should be automatically modified if we use usethis. The DESCRIPTION is our first stop. You should make yourself the author, Authors@R: person(&quot;given name&quot;, &quot;family name&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;), &quot;contact@mail.com&quot;) The roles aut and cre stand for author and creator respectively. For persons with an ORCID identifier (see https://orcid.org/, you can provide the identifier as part of the comment argument of person() person(&quot;&quot;given name&quot;, &quot;family name&quot;, comment = c(ORCID = &quot;0000-1111-2222-3333&quot;)) More details regarding use of roles can be found here. Next you need to fill in the Title and Description fields. The Title should be a short and simple description of the package. Must be plain text, title-case (This Sentence Is Title Case), not end in a period and should preferable be less then 65 characters. The Description fields is the more detailed version of the Title field. It should span multiple lines, each indented 4 spaces (first excluded) and being less then 80 characters long. This is a fairly limited space so further details should be included in the readme. The package created here doesn’t provide anything in terms of functions or data. Adding these objects will be the subject of the next section. 3.4 Additional Components This section will introduce various components you can include in your package to improve its functionality and usability. 3.4.1 R code R/ The meat of a package will generally come from the functions it provides (exceptions are data packages which just contain data, the gapminder is a notable example of this). All your functions should be includes inside the R/ folder in .R scripts. While there are no hard rules regarding the organization of functions, it is generally advised to divide the functions among multiple .R scripts for better organization. Likewise will naming the scripts not improve the run time and efficiency of your code but will make it easier to navigate. A .R script can be created manually and saved in the R/ folder, or you can use the use_r() function from usethis. Suppose we are adding a function to cluster some data according to the k-means clustering, an appropriate name of the .R script could be kmeans. use_r(&quot;kmeans&quot;) This will create a file called kmeans.R in the R/ folder. To make the functions usable you must export them, this will be covers in the next subsection Documentation. Further reading: https://r-pkgs.org/r 3.4.2 Documentation Documentation is perhaps the single most important part of a package, without it, new users would not be able to use your otherwise excellent package. the documentation in a r package is stored in the man/ folder, but you should never modify these files by hand. You should use roxygen2 to document your code in the .R scripts. roxygen2 comments looks like this #&#39; Adds 1 to a number. #&#39; #&#39; This function is vectorized so it allows vectors of any length as input. #&#39; #&#39; @param x A number. #&#39; @return 1 plus the value of \\code{x}. #&#39; @export #&#39; @examples #&#39; addone(pi) #&#39; addone(1:10) addone &lt;- function(x) { x + 1 } Notice how each like line begins with #' this is the indication that is is a roxygen2 comment to be used for documentation. A basic function documentation comes with 6 parts: The “title” is first line of a roxygen2 comment block. This becomes the headline. This should be kept brief and concise. The following paragraphs is the description and details. The first paragraph will become the description (to appear right after the title) and the remaining paragraphs will become details to appear latter on the documentation page. Argument documentation. There should be a @param tag for each argument in the function noting the name of the argument and its use. The expected outcome. The @return should be followed by a brief explanation of the type of output the function will provide. Declaration of exportation. If you want the function to be used by the user you must include the @export tag in the roxygen2 comments. Examples. Write various examples following the @examples tag. these examples should be fairly small and run fast. for information on proper use and additional tags please refer to the links in further reading. To quickly provide a roxygen2 skeleton do Code &gt; Insert roxygen skeleton in Rstudio while having the function selected. once you have written the documentation as roxygen2 comments you need to turn it into .Rd files which should be located in the man/ folder. This is done by running roxygen. There are three main ways to run roxygen: roxygen2::roxygenise(), or devtools::document(), if you’re using devtools, or Ctrl/Cmd + Shift + D, if you’re using RStudio. Further reading: https://r-pkgs.org/man https://cran.r-project.org/doc/manuals/R-exts.html#Rd-format https://ropensci.github.io/dev_guide/building.html#documentation 3.4.3 Dependencies One of the jobs of the DESCRIPTION file is to denote that other packages are needed for your package to run. Generally you will only need to use 3 of the tags. Imports, Suggests and LinkingTo. Imports are packages that your package to work. Suggests are packages that your package can use but doesn’t need. LinkingTo packages listed here rely on C or C++ code in another package. Both Imports and Suggests fields can be changed using the usethis function use_package() # Sets dplyr as &#39;Imports&#39; use_package(&quot;dplyr&quot;, type = &quot;Imports&quot;) # Sets dplyr as &#39;Suggests&#39; use_package(&quot;dplyr&quot;, type = &quot;Suggests&quot;) Once this have been done you should refer to the functions with the :: operator in the style package::function. As an example, lets say we want to calculate the procentage of a data.frame is composed of distinct rows. Here it can be advantageous to leverage the distinct() function from the dplyr package. We will start by calling use_package(&quot;dplyr&quot;, type = &quot;Imports&quot;) to make sure that dplyr is specified in the DESCRIPTION file, next we can use the distinct() function by prefixing it with dplyr::. distinct_procentage &lt;- function(df) { df_distinct &lt;- dplyr::distinct(df) full_n &lt;- nrow(df) distinct_n &lt;- nrow(df_distinct) distinct_n / full_n } The LinkingTo field will automatically filled when you are to use C or C++, this will be covered in the section on Compiled code. Further reading: https://r-pkgs.org/description.html#dependencies https://ropensci.github.io/dev_guide/building.html#package-dependencies http://dirk.eddelbuettel.com/blog/2019/03/14/#020_dependency_badges 3.4.4 License The License field in the DESCRIPTION file is telling you what kind of license applies to the code in the package. We will not cover the difference between the different licenses in the book and will refer readers to https://r-pkgs.org/description.html#license as a starting point and the further reading section for further reading. In general you will end with one of the 3 licenses MIT, GPL-3 or CC0. Once you have decided on your license you simple use the corresponding usethis function to handle the rest. use_mit_license(&quot;My Name&quot;) use_gpl3_license(&quot;My Name&quot;) use_cc0_license(&quot;My Name&quot;) Further reading: https://r-pkgs.org/description.html#license https://cran.r-project.org/doc/manuals/R-exts.html#Licensing https://blog.codinghorror.com/pick-a-license-any-license/ https://choosealicense.com/licenses/ https://tldrlegal.com/ 3.4.5 README The readme acts as a main page for your package. This will be the first thing a user will see if your package is only on github. It is still relevant once you get the package on CRAN but less so as more users will go through the CRAN page instead. The readme file is created using the usethis package Further reading: https://ropensci.github.io/dev_guide/building.html#readme 3.4.6 Tests Once you start writing functions you want to make sure that they work the way you expect them to work. More importantly you would like for them to continue working as you adjust them. This is where testing comes in. You write an expression using your function, you then write what you expect the expression to do and compare. If those two things are the same the test have passed. To do testing in R we recommend you use a framework such as testthat. A thorough guide of testing is done can be found in the testing chapter. Further reading: https://r-pkgs.org/tests.html https://ropensci.github.io/dev_guide/building.html#testing 3.4.7 Data You might want to a a small example data set to showcase how your package works. If you have the script that creates the data, start by calling the use_data_raw() function from the usethis package. This will create a folder in which you should include the script that created the data. Once you have your data use the use_data() function on the data object and usethis will take care of the rest. Further reading: https://r-pkgs.org/data.html#data 3.4.8 Compiled code 3.4.9 vignettes Vignettes are a large scale example. Use it to explain the workflow of your package. Remember that the reader don’t know the package as well as you do. Describe the problem(s) your package is trying to solve and show how it is used to solve it. Adding a vignette is done by using the use_vignette() function from the usethis package like so use_vignette(&quot;Name of my vignette&quot;) This will create a skeleton vignette that you can fill out. Further reading: https://r-pkgs.org/vignettes.html 3.5 Resources Usethis package https://usethis.r-lib.org/ Cran documentation https://cran.r-project.org/doc/manuals/R-exts.html R Packages https://r-pkgs.org/ https://www.hvitfeldt.me/blog/usethis-workflow-for-package-development/ You can make a package in 20 minutes - Rstudio Conf Talk by Jim Hester https://www.rstudio.com/resources/videos/you-can-make-a-package-in-20-minutes/ https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/ https://cran.r-project.org/doc/manuals/r-release/R-exts.html https://ropensci.github.io/dev_guide/ https://ropensci.org/blog/2018/03/16/thanking-reviewers-in-metadata/ "],
["parallel-computing.html", "4 Parallel computing 4.1 Introduction 4.2 What is parallel computing, anyway? 4.3 When is it a good idea? 4.4 Fundamentals 4.5 Parallel computing in R 4.6 parallel example 2: Simulating \\(\\pi\\)", " 4 Parallel computing Before starting, be advice that this chapter is in continuous work-in-progress mode, which translates in potential changes in the ordering of the sections, availability of contents, etc. 4.1 Introduction In this chapter we will introduce the reader to parallel computing using R. In particular, we will take a general overview on what is parallel computing, how can it benefit us, and how can it be used in R. The rest of this chapter develops under the assumption that the reader has some level of knowledge about R fundamentals (types of objects, functions, etc.). 4.2 What is parallel computing, anyway? In very simple terms, parallel computing is all about making things run faster. More concrete, while there are plenty ways of accelerating calculations, parallel computing is all about doing multiple things simultaneously. Sequential computing, on the other hand, is what we usually see in R. When we make a call to a function, most of the time R is doing calculations using a single processor. While this may not be a problem if your call takes just a couple of seconds, it may be critical if, for example, the program needs to make a significant number of such calls, say 1,000, in order to be completed. While not a general rule, most of the time computationally intensive programs can be split in such a way that its components can be executed in a isolated way, this is, without relying in the other parts to be completed. For example, suppose that you have a function that takes a number and multiplies it by 2, let’s call it f: f &lt;- function(n) n*2 In R, this simple function can be applied seamlessly to a vector, for example: x &lt;- 1:4 f(x) ## [1] 2 4 6 8 If we were able to see how calculations are taking place in R, we could represent this in the following way: Here we are using a single core. The function is applied one element at a time, leaving the other 3 cores without usage. Now, in a parallel computing setting, the same function can be applied simultaneously to multiple elements of x, as in the following figure, where the number of cores matches the number of elements/tasks that need to be processed: In this more intelligent way of computation, we are taking full advantage of our computer by using all 4 cores at the same time. This will translate in a reduced computation time which, in the case of complicated/long calculations, can be an important speed gain. In principle, this implementation of the function f should take 1/4 of what the original version takes to be applied to x. As the number of function calls increases, or in other words, as the complexity in terms of computational time increases, it makes sense to start thinking about parallel computing. This takes us to the next section. 4.3 When is it a good idea? Parallel computing sounds great, but is it always a good idea to try to parallelize your program? The answer is no. A lot of times we feel pushed towards writing the code as efficient as possible. Moreover, this is a known problem among software developers, see for example this hilarious reply on Stackoverflow regarding “bets comment in source code”1: // // Dear maintainer: // // Once you are done trying to &#39;optimize&#39; this routine, // and have realized what a terrible mistake that was, // please increment the following counter as a warning // to the next guy: // // total_hours_wasted_here = 42 // While optimizing a program may be important in some cases, time constraints and readability may be more important in relative terms. As a rule of thumb, you will only want to optimize your code if by doing so the potential speed gains are worthwhile, for example, reducing computation speed to half of the original time in a algorithm that takes more than just a few seconds. Other examples include: Good idea when: You are writing a log-likelihood Function that you need to maximize. Solvers take, for example, at least 5 calls to the objective function so it makes sense to speed up the call. Even more relevant than a simple optimization, if the function needs to be called thousands of times like in a MCMC algorithm, then it definitely makes sense to improve speed. You are processing chunks of data in which each requires a significant amount of time to be processed. Bad idea when: The section of the program that you want to speed up already takes a relatively small time to be completed, for example, a few seconds or a fraction of a second. The section of the program you are trying to optimize is not the the actual bottle neck of the program2 If your computational problem is reasonable enough to think about code optimization, and furthermore, implementing parallel computing, then the following diagram should be a useful guide to follow: Ask yourself these questions before jumping into HPC! If your problem reached the part in which it can be parallelized but there are no tools around for you to use, keep reading, otherwise move to the next chapter and don’t come back until you have a problem worthy enough to be dealt with parallel computing… just kidding. 4.4 Fundamentals Before jumping into HPC with R, let’s take a look at some concepts that are fundamental for the rest of the chapter. 4.4.1 Types of parallelisms A nice way to look at types of computation is through Flynn’s taxonomy: Flynn’s Classical Taxonomy. Source: Introduction to Parallel Computing, Blaise Barney, Lawrence Livermore National Laboratory (website) 4.4.2 What you need to know about Hardware One important thing to know is how many resources we have, and resources can be very different across systems. In general, we can talk about a computer’s processing unit, CPU, as a collection of cores which are grouped/arranged in sockets. More over, modern CPUs such as those built by intel have what they call multithreaded technology, which in raw terms means a single physical core behaving as multiple ones. The following figure shows a nice illustration of this: Taxonomy of CPUs (Downloaded from de https://slurm.schedmd.com/mc_support.html) Now, how many cores does your computer has, the parallel package can tell you that: parallel::detectCores() ## [1] 4 4.4.3 HPC in R Loosely, from R’s perspective, we can think of HPC in terms of two, maybe three things: Big data: How to work with data that doesn’t fit your computer Parallel computing: How to take advantage of multiple core systems Compiled code: Write your own low-level code (if R doesn’t has it yet…) In the case of Big Data, some solutions include: Buy a bigger computer/RAM memory (not the best solution!) Use out-of-memory storage, i.e., don’t load all your data in the RAM. e.g. The bigmemory, data.table, HadoopStreaming R packages Store it more efficiently, e.g.: Sparse Matrices (take a look at the dgCMatrix objects from the Matrix R package) 4.5 Parallel computing in R As mentioned earlier, R was not designed to work with parallel computing out-of-the-box. While there are some ways to go around this such as: Obtaining the R version owned by Microsoft (Microsoft R Open), which has some features, and in particular, linear algebra routines compiled in parallel; Compiling R with BLAS allowing for parallel computing (a couple of examples here and here); Getting the opensource version pqR (pretty quick R, which at the writing of this has a stable release published on February 19th, 2019); When it comes to use “normal” R, there are several alternatives (just take a look at the High-Performance Computing Task View). Here we will focus on the following R-packages/tools for explicit parallelism: parallel: R package that provides ‘[s]upport for parallel computation, including random-number generation’. RcppArmadillo + OpenMP RcppArmadillo: ‘Armadillo is a C++ linear algebra library, aiming towards a good balance between speed and ease of use.’ ‘[RcppArmadillo] brings the power of Armadillo to R.’ OpenMP: ‘Open Multi-Processing is an application programming interface (API) that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran, on most platforms, processor architectures and operating systems, including Solaris, AIX, HP-UX, Linux, macOS, and Windows.’ (Wiki) Implicit parallelism, on the other hand, are out-of-the-box tools that allow the programmer not to worry about parallelization, e.g. such as gpuR for Matrix manipulation using GPU. 4.5.1 The parallel package Create a cluster: PSOCK Cluster: makePSOCKCluster: Creates brand new R Sessions (so nothing is inherited from the master), even in other computers! Fork Cluster: makeForkCluster: Using OS Forking, copies the current R session locally (so everything is inherited from the master up to that point). Not available on Windows. Other: makeCluster passed to snow Copy/prepare each R session: Copy objects with clusterExport Pass expressions with clusterEvalQ Set a seed Do your call: mclapply, mcmapply if you are using Fork parApply, parLapply, etc. if you are using PSOCK Stop the cluster with clusterStop 4.5.2 parallel example 1: Parallel RNG # 1. CREATING A CLUSTER library(parallel) cl &lt;- makePSOCKcluster(2) # 2. PREPARING THE CLUSTER clusterSetRNGStream(cl, 123) # Equivalent to `set.seed(123)` # 3. DO YOUR CALL ans &lt;- parSapply(cl, 1:2, function(x) runif(1e3)) (ans0 &lt;- var(ans)) # [,1] [,2] # [1,] 0.0861888293 -0.0001633431 # [2,] -0.0001633431 0.0853841838 # I want to get the same! clusterSetRNGStream(cl, 123) ans1 &lt;- var(parSapply(cl, 1:2, function(x) runif(1e3))) ans0 - ans1 # A matrix of zeros # [,1] [,2] # [1,] 0 0 # [2,] 0 0 # 4. STOP THE CLUSTER stopCluster(cl) In the case of makeForkCluster # 1. CREATING A CLUSTER library(parallel) # The fork cluster will copy the -nsims- object nsims &lt;- 1e3 cl &lt;- makeForkCluster(2) # 2. PREPARING THE CLUSTER RNGkind(&quot;L&#39;Ecuyer-CMRG&quot;) set.seed(123) # 3. DO YOUR CALL ans &lt;- do.call(cbind, mclapply(1:2, function(x) { runif(nsims) # Look! we use the nsims object! # This would have fail in makePSOCKCluster # if we didn&#39;t copy -nsims- first. })) (ans0 &lt;- var(ans)) # [,1] [,2] # [1,] 0.08538418 0.00239079 # [2,] 0.00239079 0.08114219 # Same sequence with same seed set.seed(123) ans1 &lt;- var(do.call(cbind, mclapply(1:2, function(x) runif(nsims)))) ans0 - ans1 # A matrix of zeros # [,1] [,2] # [1,] 0 0 # [2,] 0 0 # 4. STOP THE CLUSTER stopCluster(cl) 4.6 parallel example 2: Simulating \\(\\pi\\) We know that \\(\\pi = \\frac{A}{r^2}\\). We approximate it by randomly adding points \\(x\\) to a square of size 2 centered at the origin. So, we approximate \\(\\pi\\) as \\(\\Pr\\{\\|x\\| \\leq 1\\}\\times 2^2\\) The R code to do this pisim &lt;- function(i, nsim) { # Notice we don&#39;t use the -i- # Random points ans &lt;- matrix(runif(nsim*2), ncol=2) # Distance to the origin ans &lt;- sqrt(rowSums(ans^2)) # Estimated pi (sum(ans &lt;= 1)*4)/nsim } # Setup cl &lt;- makePSOCKcluster(10) clusterSetRNGStream(cl, 123) # Number of simulations we want each time to run nsim &lt;- 1e5 # We need to make -nsim- and -pisim- available to the # cluster clusterExport(cl, c(&quot;nsim&quot;, &quot;pisim&quot;)) # Benchmarking: parSapply and sapply will run this simulation # a hundred times each, so at the end we have 1e5*100 points # to approximate pi rbenchmark::benchmark( parallel = parSapply(cl, 1:100, pisim, nsim=nsim), serial = sapply(1:100, pisim, nsim=nsim), replications = 1 )[,1:4] # test replications elapsed relative # 1 parallel 1 0.302 1.000 # 2 serial 1 1.556 5.152 ans_par &lt;- parSapply(cl, 1:100, pisim, nsim=nsim) ans_ser &lt;- sapply(1:100, pisim, nsim=nsim) stopCluster(cl) # par ser R # 3.141561 3.141247 3.141593 4.6.1 Speedup things with Rcpp + OpenMP Read the original post (now closed) here.↩ For more about how to identify code bottlenecks, take a look at the Profiling section of this book here.↩ "],
["testing.html", "5 Testing", " 5 Testing "],
["profile-benchmark.html", "6 Profiling and benchmarking 6.1 Introduction 6.2 Profiling 6.3 Benchmarking 6.4 Additional resources", " 6 Profiling and benchmarking 6.1 Introduction This chapter will introduce the the concepts of profiling and benchmarking. These concepts are universal withing programming . This chapter will focus on its practical implementation and usage in the R programming language. The overall goal of these techniques is to measure the performance of the code you have written. Remember this is a measure of speed, not a measure of correctness. 6.2 Profiling Profiling is the act of measuring the run-time of each line of code you have run. Knowing where the time is being spend in your code is beneficial as it is a good indication of where you should spend your time optimizing. In general you want to look for small areas that take up most of the time (also called a “bottleneck”) and focus on those before other parts. There is little reason to spend time optimizing a piece of code that only take up 0.1% of the time when you could work at a piece that takes up 70% of the time. We will use the the profvis package to do profiling. It have a couple of different ways of interacting. In the first one you load the profvis package and then you wrap the code you want to profile in profvis({ and }) as shown below show below. library(profvis) profvis({ data &lt;- runif(1e7) # Three different ways of getting the square root square_root &lt;- sqrt(data) square_root &lt;- data ^ (1/2) square_root &lt;- exp(1) ^ (1/2 * log(data)) }) Another way if you are using the Rstudio IDE, comes from the navigation bar where you can access the profiling tool. Figure 6.1: Profile location in Rstudio IDE navigation bar. Clicking this tab reveals the following actions: “Profile selected line(s)” “Start profiling” &amp; “Stop profiling” Figure 6.2: Profiling actions. Being able to profile selected lines of code is great if you have a short and compact piece of code that easily can be highlighted and tested. On the other hand is the ability start and stop the profiling whenever you want a powerful tool. In addition to being able to profile code from different areas, you are also able to stop profiling before the code is done executing, which you aren’t able to in the previous 2 methods. This is useful if you want to profile a snapshot of a long-running simulation as it can have very consistent behavior since it is running the same thing millions of time. No matter which of the three way you do your profile you will be presented with a page with a frame-graph Figure 6.3: profvis output showing source on top and flame graph below. This interactive panel shows how much time and memory is being spend on each line of code. From here you should be able to identify Another useful view can be found by clicking on the “data” tab at the top. This shows how long time is being spend in each expression. We can see in this example that the power operator ^ is taking the majority of the time. Figure 6.4: profvis data view showcases results by expresion in stead of by line. 6.2.1 Troubleshooting Sometimes when you are using profvis you will see the error Error in parse_rprof(prof_output, expr_source) : No parsing data available. Maybe your function was too fast? This is because your code finished running before profvis was able to detect it. This might feel like good news, but it can make it difficult to profile very fast functions. To profile a fast function you simply let it run a lot of times. This can easily be done by putting it inside a for-loop. You change this profvis({ data &lt;- c(3, 7, 2) super_fast_function(data) }) to profvis({ data &lt;- c(3, 7, 2) for (i in 1:1000) { super_fast_function(data) } }) where you increase the number until it is run enough for the profiler to catch it. 6.3 Benchmarking Measuring how long something takes is a simple skill that will become invaluable once you start to focus on making your code faster. Simply put, if you can’t measure how fast something is you don’t know if it is going any faster. This section will be broken into 2 sections benchmarking slow code and, benchmarking fast code. In this content slow is something that takes seconds, minutes, hours or more. It is a situation where you could use a conventional stopwatch. Fast is anything faster, it is used in the context where you have two pieces of code you think does the same and you want to find out which one is faster. 6.3.1 Slow code First we need to create a function to benchmark, here we will use this simple recursive formula for the fibonacci sequence. This function doesn’t scale well with n so it will be perfect for these examples. fibonacci &lt;- function(n) { if(n == 0) { return(0) } if(n == 1) { return(1) } fibonacci(n - 1) + fibonacci(n - 2) } Using system.time() is classic way to measure how long something takes, simple wrap the code you want to time between system.time({ and }). system.time({ fibonacci(32) }) ## user system elapsed ## 2.740 0.000 2.744 The first two numbers are the are the total user and system CPU times of the current R process and any child processes on which it has waited, and the third entry is the ‘real’ elapsed time since the process was started. An alternative with the same functionality from the bench package is the function system_time(). library(bench) system_time({ fibonacci(32) }) ## process real ## 2.67s 2.67s where the two values are process - The process CPU usage of the expression evaluation. real - The wall clock time of the expression evaluation. Another great tool is the tictoc package. Simply call tic() when to start recording and toc() to end recording. library(tictoc) tic() x &lt;- fibonacci(32) toc() ## 2.808 sec elapsed In addition does this package extend the timing functionality in such a way that we are able to measure times in nested context. In the following example we are generating some data and fitting a model. Calling tic() another time before the toc() allows us to measure subsections of the whole. library(tictoc) tic(&quot;Total&quot;) tic(&quot;Data Generation&quot;) X &lt;- matrix(rnorm(5000 * 1000), 5000, 1000) b &lt;- sample(1:1000, 1000) y &lt;- runif(1) + X %*% b + rnorm(5000) toc() tic(&quot;Model Fitting&quot;) model &lt;- lm(y ~ X) toc() toc() ## Data Generation: 0.407 sec elapsed ## Model Fitting: 4.796 sec elapsed ## Total: 5.206 sec elapsed This can be useful if you want to be able to time the overall script as well as parts of it. Notice how each timing is named. 6.3.2 Fast code - microbenchmarking Here we will look at the what happens when we want to compare two expressions to see which one is faster. We will use the bench package again. Suppose we would like to determine the fastest way of calculating the variance of a selection of numbers. We use the mark() function from the bench and insert 2 or more expressions we would like to test against each other. These expressions are then run a lot of times and the summary statistics of the times are given as a result. library(bench) x &lt;- rnorm(1000) bench::mark( var(x), cov(x, x) ) ## # A tibble: 2 x 10 ## expression min mean median max `itr/sec` mem_alloc n_gc n_itr ## &lt;chr&gt; &lt;bch:&gt; &lt;bch:&gt; &lt;bch:&gt; &lt;bch:t&gt; &lt;dbl&gt; &lt;bch:byt&gt; &lt;dbl&gt; &lt;int&gt; ## 1 var(x) 9.5µs 11.9µs 10.7µs 103.2µs 83963. 13.7KB 1 9999 ## 2 cov(x, x) 19.5µs 22.1µs 21.5µs 77.9µs 45231. 47.6KB 4 9996 ## # … with 1 more variable: total_time &lt;bch:tm&gt; mark() also checks that all the expressions return the same output as a sanity check. Notice the units 1 ms, then one thousand calls takes a second. 1 µs, then one million calls takes a second. 1 ns, then one billion calls takes a second. 6.4 Additional resources https://adv-r.hadley.nz/perf-measure.html Chapter on “Measuring performance” from Advanced R by Hadley Wickham. Covers more or less the same topics as this chapter but with more examples and greater details, great next step for reading. "],
["hpc.html", "7 HPC 7.1 Keep yor library organized 7.2 Don’t hijack nodes 7.3 Keep tempfiles where they belong 7.4 Use links and not hard copies of large data 7.5 Be mindful about special configs for R packages", " 7 HPC Some important points to consider while using HPC in R 7.1 Keep yor library organized In a lot of setups the “Desk” space available (a.k.a. home) in HPC clusters is rather small. In such cases it makes sense to keep permanent files elsewhere. This applies directly to the R library (where R packages live). The simple tip is Keep all your good stuff in a folder where you have a large storage (rather permanent) space, and create a symbolic link to it at home. Here is an example setup: $ cd /where/i/have/space/ $ mkdir Rlibs $ cd ~ $ ln -s R /where/i/have/space/Rlibs This way, whenever you install an R package, by default it will be installed in your /where/i/have/space/Rlibs folder, but it is easy to access from any R session sin R will check (by default) your home directory for an R folder. In the particular case of USC, home directories (not to be confounded with project directories) are rather small, 1GB last time I checked, which makes sense as you shouldn’t be using the home directory for storage but for script submission. 7.2 Don’t hijack nodes A lot of times people tend to request resources by the node, which is a terrible (not very nice) practice. Instead of specifying how many nodes you want to use, tell the job-scheduler how many tasks (or cores) you request. In the case of Slurm, we can use the following setup: #SBATCH ntasks=1 #SBATCH cpus-per-task=10 The previous code is advising3 Slurm to allocate 10 cores for the job to be submitted. Another way to be more efficient is using Arrays, e.g. #SBATCH array=1-10 #SBATCH ntasks=1 #SBATCH cpus-per-task=4 This code tells Slurm that this job should be repeated 10 times with the same setup (10 jobs each with a single task, but each task using 4 processors, i.e. 40 cores). The key is in the environment variable SLURM_ARRAY_TASK_ID which will take values 1 through 10 at each job, so the user can specify what to do in each one of the jobs, for example, in R: ARRAY_ID &lt;- Sys.getenv(&quot;SLURM_ARRAY_TASK_ID&quot;) dat &lt;- readRDS(sprintf(&quot;path/to/a/file/file-number-%d.rds&quot;, ARRAY_ID)) 7.3 Keep tempfiles where they belong TL;DR use the stagging or tmp filesystems in your cluster to store temporal files. Only files that will be used for analysis (results) should be stored in project space. This means that any by-product like log files, auxiliar files, etc. should be kept in those places 7.4 Use links and not hard copies of large data A very common and bad practice is to keep copies of large files duplicated accross projects. A good practice is to have symbolic links to whatever large data set you are planning to use instead. For example, suppose that we have the following file /path/to/a/very/large/big-dataset.csv Instead of creating a copy in your project directory, you should consider doing the following instead: $ mkdir data-raw $ cd data-raw $ ln -s big-dataset.csv /path/to/a/very/large/big-dataset.csv System managers (and your future self), will appreciate it. 7.5 Be mindful about special configs for R packages Some times R packages installed from source have flags that can cause your code to blow in a HPC cluster setting. One recent example of this is the R package rstan. In this case, some flags passed to the compiler made the package to fine-tune the compilation to the current machine, which causes problems when your cluster is actually heterogenous in terms of nodes setup. In Slurm, setting the number of cores or memory required for a particular job is actually an advice since the job-scheduler will try to allocate your job in a set of nodes (or single node) that hopefully goes along with the requirement. There are ways to be more strict about some of these, but is not recomended to follow that path.↩ "]
]
